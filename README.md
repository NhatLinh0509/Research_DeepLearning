BLIP Flickr Captioning
ğŸ“Œ Giá»›i thiá»‡u
Repository nÃ y chá»©a toÃ n bá»™ mÃ£ nguá»“n vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n model BLIP Ä‘á»ƒ táº¡o caption cho áº£nh, sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Flickr30k vÃ  Flickr8k.

Flickr30k: 2 model huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u 30.000 áº£nh.

Flickr8k: 1 model huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u 8.000 áº£nh.

ğŸ“‚ Model Ä‘Ã£ huáº¥n luyá»‡n khÃ´ng Ä‘Æ°á»£c lÆ°u trá»±c tiáº¿p táº¡i Ä‘Ã¢y do giá»›i háº¡n dung lÆ°á»£ng, mÃ  Ä‘Æ°á»£c lÆ°u trÃªn Hugging Face Hub.

ğŸ”— Link 3 model trÃªn Hugging Face:

https://huggingface.co/nhatlinh59/blip-flickr-captioning/tree/main

ğŸ† Model tá»‘t nháº¥t
Model blip-finetuned-part9.zip cho káº¿t quáº£ tá»‘t nháº¥t trong toÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
ToÃ n bá»™ vÃ­ dá»¥ káº¿t quáº£ dÆ°á»›i Ä‘Ã¢y Ä‘Æ°á»£c táº¡o tá»« model nÃ y.

ğŸ“‚ Ná»™i dung repo
/notebooks: Notebook huáº¥n luyá»‡n model.

/scripts: CÃ¡c script xá»­ lÃ½ dá»¯ liá»‡u, train, vÃ  Ä‘Ã¡nh giÃ¡.

README.md: TÃ i liá»‡u nÃ y.

ğŸ›  CÃ¡ch sá»­ dá»¥ng
python
Sao chÃ©p
Chá»‰nh sá»­a
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests

processor = BlipProcessor.from_pretrained("nhatlinh59/blip-flickr30k-model1")
model = BlipForConditionalGeneration.from_pretrained("nhatlinh59/blip-flickr30k-model1")

img_url = "https://example.com/sample.jpg"
image = Image.open(requests.get(img_url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)

print(caption)
ğŸ“¸ Káº¿t quáº£ máº«u
VÃ­ dá»¥ 1
<img width="1054" height="546" alt="image" src="https://github.com/user-attachments/assets/cb806f81-0dd2-4c76-a1a6-c49aa1b5d475" />

a group of asian people posing for a picture.

VÃ­ dá»¥ 2
<img width="621" height="517" alt="image" src="https://github.com/user-attachments/assets/9b9e4fe3-c184-4c72-a313-fcc047a5c95d" />

a little girl in a gray sweater and jeans is playing on a bed.

ğŸ“Š ThÃ´ng tin huáº¥n luyá»‡n
Thuáº­t toÃ¡n: Fine-tuning BLIP (Bootstrapping Language-Image Pre-training) vá»›i Transformer Decoder.

Batch size: 4

Optimizer: AdamW

Loss: Cross-Entropy Loss

Epochs: Chia dá»¯ liá»‡u thÃ nh nhiá»u pháº§n, huáº¥n luyá»‡n theo tá»«ng pháº§n Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i bá»™ nhá»›.

