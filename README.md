BLIP Flickr Captioning
ğŸ“Œ Giá»›i thiá»‡u
Repository nÃ y chá»©a toÃ n bá»™ mÃ£ nguá»“n vÃ  quÃ¡ trÃ¬nh huáº¥n luyá»‡n model BLIP Ä‘á»ƒ táº¡o caption cho áº£nh, sá»­ dá»¥ng bá»™ dá»¯ liá»‡u Flickr30k vÃ  Flickr8k.

Flickr30k: 2 model huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u 30.000 áº£nh.

Flickr8k: 1 model huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u 8.000 áº£nh.

ğŸ“‚ Model Ä‘Ã£ huáº¥n luyá»‡n khÃ´ng Ä‘Æ°á»£c lÆ°u trá»±c tiáº¿p táº¡i Ä‘Ã¢y do giá»›i háº¡n dung lÆ°á»£ng, mÃ  Ä‘Æ°á»£c lÆ°u trÃªn Hugging Face Hub.

ğŸ”— Link 3 model trÃªn Hugging Face:

https://huggingface.co/nhatlinh59/blip-flickr-captioning/tree/main

ğŸ† Model tá»‘t nháº¥t
Model blip-finetuned-part9.zip cho káº¿t quáº£ tá»‘t nháº¥t trong toÃ n bá»™ quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
ToÃ n bá»™ vÃ­ dá»¥ káº¿t quáº£ dÆ°á»›i Ä‘Ã¢y Ä‘Æ°á»£c táº¡o tá»« model nÃ y.

ğŸ“‚ Ná»™i dung repo
/notebooks: Notebook huáº¥n luyá»‡n model.

/scripts: CÃ¡c script xá»­ lÃ½ dá»¯ liá»‡u, train, vÃ  Ä‘Ã¡nh giÃ¡.

README.md: TÃ i liá»‡u nÃ y.

ğŸ›  CÃ¡ch sá»­ dá»¥ng
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import requests

processor = BlipProcessor.from_pretrained("nhatlinh59/blip-finetuned-part9")
model = BlipForConditionalGeneration.from_pretrained("nhatlinh59/blip-finetuned-part9")

img_url = "https://example.com/sample.jpg"
image = Image.open(requests.get(img_url, stream=True).raw)

inputs = processor(images=image, return_tensors="pt")
out = model.generate(**inputs)
caption = processor.decode(out[0], skip_special_tokens=True)

print(caption)

ğŸ“¸ Káº¿t quáº£ máº«u
VÃ­ dá»¥ 1
<img width="1054" height="546" alt="image" src="https://github.com/user-attachments/assets/cb806f81-0dd2-4c76-a1a6-c49aa1b5d475" />

a group of asian people posing for a picture.

VÃ­ dá»¥ 2
<img width="621" height="517" alt="image" src="https://github.com/user-attachments/assets/9b9e4fe3-c184-4c72-a313-fcc047a5c95d" />

a little girl in a gray sweater and jeans is playing on a bed.

ğŸ“Š ThÃ´ng tin huáº¥n luyá»‡n

img_dir = "/kaggle/input/flickr-image-dataset/flickr30k_images/flickr30k_images"
batch_size = 4
epoch_per_part = 1

for i in range(num_parts):
    print(f"\nğŸ” Huáº¥n luyá»‡n pháº§n {i}")
    dataset = Flickr30kDataset(f"splits/train_part_{i}.csv", img_dir, processor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)

    model.train()
    for epoch in range(epoch_per_part):
        loop = tqdm(dataloader)
        for batch in loop:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            pixel_values = batch["pixel_values"].to(device)
            labels = input_ids.clone()

            outputs = model(pixel_values=pixel_values,
                            input_ids=input_ids,
                            attention_mask=attention_mask,
                            labels=labels)

            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            loop.set_description(f"Part {i}")
            loop.set_postfix(loss=loss.item())

    path = f"/kaggle/working/blip-finetuned-part{i}"
    model.save_pretrained(path)
    processor.save_pretrained(path)
    print(f"âœ… ÄÃ£ lÆ°u táº¡i: {path}")

